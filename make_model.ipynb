{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import models\n",
    "from utils import save_net,load_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is with CBAM attention\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    #global avg pooling and max pooling-> compress feature maps into 1x1xC\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "    #pass both tru an MLP (2 conv layer) and sum\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "    \n",
    "        self.sigmoid = nn.Sigmoid() #gives weight per channel\n",
    "\n",
    "        #multiply back into input x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "    \n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7): #use 7x7 convo\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        \n",
    "        #concatenate them -> 2xHxW\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(x))\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ca = ChannelAttention(in_planes, ratio)\n",
    "        self.sa = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.ca(x)\n",
    "        x = x * self.sa(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    #sequence of convo and pooling layers\n",
    "def make_layers(cfg, in_channels=3, batch_norm=False, dilation=False):\n",
    "    d_rate = 2 if dilation else 1\n",
    "    layers = []\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate, dilation=d_rate)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class CSRNet_CBAM(nn.Module):\n",
    "    def __init__(self, load_weights=False):\n",
    "        super(CSRNet_CBAM, self).__init__()\n",
    "        self.seen = 0\n",
    "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
    "        self.backend_feat = [512, 512, 512, 256, 128, 64]\n",
    "\n",
    "        self.frontend = make_layers(self.frontend_feat)\n",
    "        self.cbam = CBAM(512)  # Add CBAM after frontend output\n",
    "        self.backend = make_layers(self.backend_feat, in_channels=512, dilation=True)\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "        if not load_weights:\n",
    "            mod = models.vgg16(pretrained=True)\n",
    "            self._initialize_weights()\n",
    "            vgg_state = mod.features.state_dict()\n",
    "            frontend_state = self.frontend.state_dict()\n",
    "            matched_weights = {k: v for k, v in vgg_state.items() if k in frontend_state and v.size() == frontend_state[k].size()}\n",
    "            frontend_state.update(matched_weights)\n",
    "            self.frontend.load_state_dict(frontend_state)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.cbam(x)        # Apply CBAM here\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## this is original \n",
    "\"\"\"\n",
    "class CSRNet(nn.Module):\n",
    "    def __init__(self, load_weights=False):\n",
    "        super(CSRNet, self).__init__()\n",
    "        self.frontend_feat = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512]\n",
    "        self.backend_feat  = [512, 512, 512, 256, 128, 64]\n",
    "\n",
    "        self.frontend = make_layers(self.frontend_feat)\n",
    "        self.backend = make_layers(self.backend_feat, in_channels=512, dilation=True)\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "        if not load_weights:\n",
    "            mod = models.vgg16(pretrained=True)\n",
    "            self._initialize_weights()\n",
    "            frontend_items = list(self.frontend.state_dict().items())\n",
    "            mod_items = list(mod.state_dict().items())\n",
    "            for i in range(len(frontend_items)):\n",
    "                frontend_items[i][1].data[:] = mod_items[i][1].data[:]\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def make_layers(cfg, in_channels = 3,batch_norm=False,dilation = False):\n",
    "    if dilation:\n",
    "        d_rate = 2\n",
    "    else:\n",
    "        d_rate = 1\n",
    "    layers = []\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate,dilation = d_rate)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#this part is to trained original csrnet with only 20 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 31, 31])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CSRNet()\n",
    "x = torch.rand((1,3,255,255))\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/elicer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/elicer/.local/lib/python3.10/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch 0, processed 0 samples, lr 0.0000100000\n",
      "üìÅ Epoch 0\n",
      "‚úÖ Total training images: 80\n",
      "üì¶ Batch size: 1\n",
      "üîÅ Total batches per epoch: 80\n",
      "üöÄ Learning rate: 0.0000100000\n",
      "Epoch: [0][0/80]\tTime 10.476 (10.476)\tData 0.023 (0.023)\tLoss 209.2738 (209.2738)\t\n",
      "Epoch: [0][30/80]\tTime 1.799 (7.705)\tData 0.005 (0.012)\tLoss 211.3284 (886.5945)\t\n",
      "Epoch: [0][60/80]\tTime 9.808 (7.868)\tData 0.012 (0.012)\tLoss 440.2245 (1063.1822)\t\n",
      "begin test\n",
      " * MAE 355.681 \n",
      " * best MAE 355.681 \n",
      "epoch 1, processed 80 samples, lr 0.0000100000\n",
      "üìÅ Epoch 1\n",
      "‚úÖ Total training images: 80\n",
      "üì¶ Batch size: 1\n",
      "üîÅ Total batches per epoch: 80\n",
      "üöÄ Learning rate: 0.0000100000\n",
      "Epoch: [1][0/80]\tTime 6.601 (6.601)\tData 0.056 (0.056)\tLoss 2679.7339 (2679.7339)\t\n",
      "Epoch: [1][30/80]\tTime 1.798 (7.533)\tData 0.005 (0.013)\tLoss 200.2495 (728.4380)\t\n",
      "Epoch: [1][60/80]\tTime 10.377 (7.649)\tData 0.013 (0.012)\tLoss 792.8362 (1128.2477)\t\n",
      "begin test\n",
      " * MAE 157.011 \n",
      " * best MAE 157.011 \n"
     ]
    }
   ],
   "source": [
    "!python train.py plain_train.json plain_val.json 0 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this part is for training if we make change to prerprocess data! PRETRAINED is at another file go find it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CSRNet_CBAM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.rand((1,3,255,255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 31, 31])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/elicer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/elicer/.local/lib/python3.10/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch 0, processed 0 samples, lr 0.0000100000\n",
      "üìÅ Epoch 0\n",
      "‚úÖ Total training images: 80\n",
      "üì¶ Batch size: 1\n",
      "üîÅ Total batches per epoch: 80\n",
      "üöÄ Learning rate: 0.0000100000\n",
      "Epoch: [0][0/80]\tTime 5.228 (5.228)\tData 0.019 (0.019)\tLoss 332.6791 (332.6791)\t\n",
      "Epoch: [0][30/80]\tTime 7.246 (8.567)\tData 0.012 (0.013)\tLoss 26.2318 (958.7766)\t\n",
      "Epoch: [0][60/80]\tTime 3.111 (8.493)\tData 0.006 (0.013)\tLoss 276.2756 (1013.9602)\t\n",
      "begin test\n",
      " * MAE 1136.261 \n",
      " * best MAE 1136.261 \n",
      "epoch 1, processed 80 samples, lr 0.0000100000\n",
      "üìÅ Epoch 1\n",
      "‚úÖ Total training images: 80\n",
      "üì¶ Batch size: 1\n",
      "üîÅ Total batches per epoch: 80\n",
      "üöÄ Learning rate: 0.0000100000\n",
      "Epoch: [1][0/80]\tTime 9.437 (9.437)\tData 0.018 (0.018)\tLoss 1500.4268 (1500.4268)\t\n",
      "Epoch: [1][30/80]\tTime 3.436 (7.995)\tData 0.007 (0.013)\tLoss 6039.8164 (1029.2448)\t\n",
      "Epoch: [1][60/80]\tTime 10.980 (7.690)\tData 0.016 (0.012)\tLoss 497.2900 (956.6684)\t\n",
      "begin test\n",
      " * MAE 147.844 \n",
      " * best MAE 147.844 \n"
     ]
    }
   ],
   "source": [
    "!python train.py plain_train.json plain_val.json 0 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/elicer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/elicer/.local/lib/python3.10/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch 0, processed 0 samples, lr 0.0000001000\n",
      "Epoch: [0][0/1540]\tTime 11.481 (11.481)\tData 0.028 (0.028)\tLoss 1240.0499 (1240.0499)\t\n",
      "Epoch: [0][30/1540]\tTime 7.477 (8.316)\tData 0.018 (0.013)\tLoss 406.8138 (1519.9125)\t\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python train.py Atest1_train.json Atest1_val.json 0 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!!! here want to try train part A with CBAM \n",
    "# freezing the frotend with the pretrained weight\n",
    "#so only traind cbam and backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 31, 31])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CSRNet_CBAM()\n",
    "x = torch.rand((1,3,255,255))\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "‚ùå frontend.0.weight\n",
      "‚ùå frontend.0.bias\n",
      "‚ùå frontend.2.weight\n",
      "‚ùå frontend.2.bias\n",
      "‚ùå frontend.5.weight\n",
      "‚ùå frontend.5.bias\n",
      "‚ùå frontend.7.weight\n",
      "‚ùå frontend.7.bias\n",
      "‚ùå frontend.10.weight\n",
      "‚ùå frontend.10.bias\n",
      "‚ùå frontend.12.weight\n",
      "‚ùå frontend.12.bias\n",
      "‚ùå frontend.14.weight\n",
      "‚ùå frontend.14.bias\n",
      "‚ùå frontend.17.weight\n",
      "‚ùå frontend.17.bias\n",
      "‚ùå frontend.19.weight\n",
      "‚ùå frontend.19.bias\n",
      "‚ùå frontend.21.weight\n",
      "‚ùå frontend.21.bias\n",
      "‚úÖ cbam.ca.fc.0.weight\n",
      "‚úÖ cbam.ca.fc.2.weight\n",
      "‚úÖ cbam.sa.conv.weight\n",
      "‚úÖ backend.0.weight\n",
      "‚úÖ backend.0.bias\n",
      "‚úÖ backend.2.weight\n",
      "‚úÖ backend.2.bias\n",
      "‚úÖ backend.4.weight\n",
      "‚úÖ backend.4.bias\n",
      "‚úÖ backend.6.weight\n",
      "‚úÖ backend.6.bias\n",
      "‚úÖ backend.8.weight\n",
      "‚úÖ backend.8.bias\n",
      "‚úÖ backend.10.weight\n",
      "‚úÖ backend.10.bias\n",
      "‚úÖ output_layer.weight\n",
      "‚úÖ output_layer.bias\n"
     ]
    }
   ],
   "source": [
    "# ===== LOAD PRETRAINED FRONTEND WEIGHTS =====\n",
    "checkpoint = torch.load('PartAmodel_best.pth.tar', map_location='cpu', weights_only=False)\n",
    "pretrained_state = checkpoint['state_dict']\n",
    "model_dict = model.state_dict()\n",
    "pretrained_dict = {k: v for k, v in pretrained_state.items() if k in model_dict and 'frontend' in k}\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "# ===== FREEZE FRONTEND =====\n",
    "for param in model.frontend.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# ===== OPTIMIZER (only trainable params) =====\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-5\n",
    ")\n",
    "\n",
    "# ===== OPTIONAL: print trainable parameters =====\n",
    "print(\"Trainable parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{'‚úÖ' if param.requires_grad else '‚ùå'} {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/elicer/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/elicer/.local/lib/python3.10/site-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch 0, processed 0 samples, lr 0.0000100000\n",
      "üìÅ Epoch 0\n",
      "‚úÖ Total training images: 770\n",
      "üì¶ Batch size: 1\n",
      "üîÅ Total batches per epoch: 770\n",
      "üöÄ Learning rate: 0.0000100000\n",
      "Epoch: [0][0/770]\tTime 4.439 (4.439)\tData 0.019 (0.019)\tLoss 950.9924 (950.9924)\t\n",
      "Epoch: [0][30/770]\tTime 1.542 (7.000)\tData 0.004 (0.012)\tLoss 366.3684 (1034.0290)\t\n",
      "Epoch: [0][60/770]\tTime 1.888 (7.644)\tData 0.005 (0.012)\tLoss 561.8679 (777.5237)\t\n",
      "Epoch: [0][90/770]\tTime 2.836 (7.704)\tData 0.007 (0.012)\tLoss 6534.6318 (987.4860)\t\n",
      "Epoch: [0][120/770]\tTime 7.676 (7.801)\tData 0.013 (0.012)\tLoss 181.0604 (1053.1652)\t\n",
      "Epoch: [0][150/770]\tTime 3.728 (7.735)\tData 0.008 (0.012)\tLoss 260.7239 (1165.3106)\t\n",
      "Epoch: [0][180/770]\tTime 2.780 (7.876)\tData 0.006 (0.012)\tLoss 1698.8020 (1154.3618)\t\n",
      "Epoch: [0][210/770]\tTime 2.522 (7.656)\tData 0.006 (0.012)\tLoss 90.7918 (1169.0286)\t\n",
      "Epoch: [0][240/770]\tTime 10.022 (7.632)\tData 0.014 (0.012)\tLoss 1207.0767 (1180.6211)\t\n",
      "Epoch: [0][270/770]\tTime 10.036 (7.722)\tData 0.013 (0.012)\tLoss 3088.7578 (1163.4990)\t\n",
      "Epoch: [0][300/770]\tTime 9.925 (7.739)\tData 0.013 (0.012)\tLoss 301.9507 (1203.1212)\t\n",
      "Epoch: [0][330/770]\tTime 4.508 (7.718)\tData 0.009 (0.012)\tLoss 2249.8296 (1205.7846)\t\n",
      "Epoch: [0][360/770]\tTime 5.459 (7.707)\tData 0.011 (0.012)\tLoss 68.7038 (1207.2286)\t\n",
      "Epoch: [0][390/770]\tTime 7.729 (7.758)\tData 0.012 (0.012)\tLoss 958.0020 (1204.6889)\t\n",
      "Epoch: [0][420/770]\tTime 1.548 (7.828)\tData 0.005 (0.012)\tLoss 987.2376 (1241.3914)\t\n",
      "Epoch: [0][450/770]\tTime 10.107 (7.776)\tData 0.016 (0.012)\tLoss 64.6757 (1274.1108)\t\n",
      "Epoch: [0][480/770]\tTime 8.321 (7.811)\tData 0.013 (0.012)\tLoss 1098.7915 (1265.9830)\t\n",
      "Epoch: [0][510/770]\tTime 11.422 (7.859)\tData 0.018 (0.012)\tLoss 265.8017 (1343.0870)\t\n",
      "Epoch: [0][540/770]\tTime 9.973 (7.908)\tData 0.015 (0.012)\tLoss 568.3716 (1333.8381)\t\n",
      "Epoch: [0][570/770]\tTime 8.411 (7.864)\tData 0.013 (0.012)\tLoss 1095.7457 (1338.9337)\t\n",
      "Epoch: [0][600/770]\tTime 2.437 (7.860)\tData 0.006 (0.012)\tLoss 520.4878 (1305.8471)\t\n",
      "Epoch: [0][630/770]\tTime 6.105 (7.830)\tData 0.012 (0.012)\tLoss 10326.8398 (1311.5163)\t\n",
      "Epoch: [0][660/770]\tTime 3.672 (7.798)\tData 0.008 (0.012)\tLoss 456.2193 (1352.7657)\t\n",
      "Epoch: [0][690/770]\tTime 11.379 (7.820)\tData 0.014 (0.012)\tLoss 1653.7927 (1393.1420)\t\n",
      "Epoch: [0][720/770]\tTime 2.664 (7.808)\tData 0.006 (0.012)\tLoss 23.9872 (1400.2871)\t\n",
      "Epoch: [0][750/770]\tTime 8.443 (7.820)\tData 0.014 (0.012)\tLoss 838.5423 (1400.9664)\t\n",
      "begin test\n",
      " * MAE 265.181 \n",
      " * best MAE 265.181 \n",
      "epoch 1, processed 770 samples, lr 0.0000100000\n",
      "üìÅ Epoch 1\n",
      "‚úÖ Total training images: 770\n",
      "üì¶ Batch size: 1\n",
      "üîÅ Total batches per epoch: 770\n",
      "üöÄ Learning rate: 0.0000100000\n",
      "Epoch: [1][0/770]\tTime 7.646 (7.646)\tData 0.012 (0.012)\tLoss 931.3326 (931.3326)\t\n",
      "Epoch: [1][30/770]\tTime 2.905 (6.573)\tData 0.008 (0.011)\tLoss 6427.2051 (1776.4700)\t\n",
      "Epoch: [1][60/770]\tTime 9.838 (6.902)\tData 0.014 (0.011)\tLoss 2962.4414 (1866.5596)\t\n",
      "Epoch: [1][90/770]\tTime 11.192 (7.214)\tData 0.014 (0.011)\tLoss 776.6680 (1538.0841)\t\n",
      "Epoch: [1][120/770]\tTime 7.557 (7.554)\tData 0.011 (0.012)\tLoss 3334.1221 (1504.1898)\t\n",
      "Epoch: [1][150/770]\tTime 9.914 (7.618)\tData 0.017 (0.012)\tLoss 446.8091 (1499.0965)\t\n",
      "Epoch: [1][180/770]\tTime 8.746 (7.789)\tData 0.013 (0.012)\tLoss 22.4978 (1499.5965)\t\n",
      "Epoch: [1][210/770]\tTime 8.359 (7.979)\tData 0.015 (0.012)\tLoss 299.5604 (1582.1140)\t\n",
      "Epoch: [1][240/770]\tTime 8.775 (7.941)\tData 0.016 (0.012)\tLoss 4597.6470 (1479.8183)\t\n",
      "Epoch: [1][270/770]\tTime 3.496 (7.876)\tData 0.008 (0.012)\tLoss 686.6166 (1400.3494)\t\n",
      "Epoch: [1][300/770]\tTime 2.442 (7.878)\tData 0.006 (0.012)\tLoss 80.3928 (1372.2375)\t\n",
      "Epoch: [1][330/770]\tTime 10.739 (7.817)\tData 0.019 (0.012)\tLoss 467.1141 (1411.7892)\t\n",
      "Epoch: [1][360/770]\tTime 11.180 (7.787)\tData 0.016 (0.012)\tLoss 11908.4697 (1466.0642)\t\n",
      "Epoch: [1][390/770]\tTime 3.481 (7.832)\tData 0.007 (0.012)\tLoss 738.2289 (1407.6857)\t\n",
      "Epoch: [1][420/770]\tTime 1.924 (7.816)\tData 0.005 (0.012)\tLoss 343.9278 (1355.1488)\t\n",
      "Epoch: [1][450/770]\tTime 6.344 (7.732)\tData 0.010 (0.012)\tLoss 330.9032 (1310.0716)\t\n",
      "Epoch: [1][480/770]\tTime 1.896 (7.738)\tData 0.005 (0.012)\tLoss 1819.7137 (1321.5653)\t\n",
      "Epoch: [1][510/770]\tTime 11.227 (7.769)\tData 0.014 (0.012)\tLoss 317.3418 (1303.2512)\t\n",
      "Epoch: [1][540/770]\tTime 6.790 (7.768)\tData 0.013 (0.012)\tLoss 228.1703 (1312.2866)\t\n",
      "Epoch: [1][570/770]\tTime 4.221 (7.758)\tData 0.009 (0.012)\tLoss 1089.3848 (1381.3583)\t\n",
      "Epoch: [1][600/770]\tTime 11.214 (7.735)\tData 0.018 (0.012)\tLoss 760.9594 (1388.0166)\t\n",
      "Epoch: [1][630/770]\tTime 2.230 (7.749)\tData 0.005 (0.012)\tLoss 127.2789 (1393.5134)\t\n",
      "Epoch: [1][660/770]\tTime 9.593 (7.753)\tData 0.016 (0.012)\tLoss 152.6289 (1442.4483)\t\n",
      "Epoch: [1][690/770]\tTime 1.852 (7.710)\tData 0.005 (0.012)\tLoss 70.0084 (1437.5341)\t\n",
      "Epoch: [1][720/770]\tTime 11.167 (7.757)\tData 0.014 (0.012)\tLoss 149.9565 (1405.1546)\t\n",
      "Epoch: [1][750/770]\tTime 9.910 (7.768)\tData 0.014 (0.012)\tLoss 104.8484 (1400.1272)\t\n",
      "begin test\n",
      " * MAE 1013.135 \n",
      " * best MAE 265.181 \n"
     ]
    }
   ],
   "source": [
    "!python train.py Atest1_train.json Atest1_val.json 0 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CBAM', 'CSRNet', 'CSRNet_CBAM', 'ChannelAttention', 'SpatialAttention', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'load_net', 'make_layers', 'models', 'nn', 'save_net', 'torch']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import model\n",
    "importlib.reload(model)  # üîÅ reload the updated model.py\n",
    "import model\n",
    "print(dir(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
